# 一、AI大模型原理与API使用

## AI分类
- 分析式AI：其核心任务是对已有数据进行
- 生成式AI：专注于创造新内容，突破在于其创造性和灵活性，但也面临数据隐私、版权保护等

## AI应用场景
- LLM大语言模型
- 文生图/文生视频（https://www.liblib.art/ 即梦AI）
- 视觉识别模型（图像分类、物体检测、图像分割等，模型如YOLO、ResNet）
- 自动驾驶

## LLM是如何训练的
- 步骤1：收集数据，微调监督模式
- 步骤2：收集比较数据，训练奖励模型（强化学习，机器学习为主，人给出评价好与不好，利用排序结果训练奖励模型）
- 步骤3：收集数据，强化学习优化模型（AI自主生成，利用奖励模型给出最好答案）

- RankList数据标记

## LLM中的Token
Token是大语言模型处理文本的最小单元。由于模型本身无法直接理解文字，因此需要将文本切分成一个个Token，再将Token转换为数字(向量)进行运算，不同模型使用不同的分词器Tokenizer（https://tiktokenizer.vercel.app/）

- 分隔符：用于区分不同的文本段落或角色，如 <|user|> 和 <|assistant|> 
- 结束符：告知模型文本已经结束，可以停止生成，如 [EOS] 或 <|endoftext|>
- 起始符：标记序列的开始，如 [CLS] (Classification) 或 [BOS] (Beginning of Sentence)，帮助模型准备开始处理文本

## Temperature与Top-P的作用
控制大模型生成文本的多样性，但原理不同。

- Temperature：在模型计算出下一个Token所有可能的概率分布后，Temperature会调整这个分布的平滑度。
  - 高Temperature：会让低概率的Token更容易被选中，使生成结果更具有创造性，可能会先不连贯的词语。
  - 低Temperature：会让高概率的Token权重更大，使生成结果更稳定、更符合训练数据，但会更保守。
- Top-P：它设置一个概率阈值，然后从高到第累加所有Token的概率，直到总和超过P未知。模型只会在这个累加出来的"核心"词汇中选择下一个Token。
  - 高Top P：候选词汇表较大，结果更多样。
  - 第Top P：候选词汇表非常小，结果更具确定性。

相比Temperature，Top P能更动态地调整候选词的数量，避免选到概率极低的离谱词汇，产生更高质量的文本。

## 大模型超能力
- 联网搜索：Function Call
- 读取文件：RAG，Chunk size大于500，
- 记忆功能：短期记忆或上下文窗口（前几轮问答作为背景信息）、长期记忆（存储在用户专属数据库中，在后续的对话中，系统会先从数据库中读取，为模型提供更为个性化的背景知识）

## 大模型API使用

- 系统提示此与用户提示词
  - 系统提示词system
    - 用于设定AI的角色、行为准则和输出格式，使贯穿对话的全局指令，为其提供了扮演的"人设"。
    - 应在对话开始时设定，内容要清晰具体
    - 它会像普通问题一样消耗Token，不应在其中包含用户的具体问题，频繁更改可能会导致AI行为不稳定
  - 用户提示词user
  - 助手assistant：助手返回的内容
  - tool：工具返回的结果

- LLM的输入与输出Token限制
  - 输入Token
    - 模型单词API调用能够处理的最大信息量，包含系统提示词、历史对话和当前用户输入的所有内容
    - 所有输入内容的总长度不能超过此限制，否则API请求会报错
    - 用户自己管理历史对话长度，
  - 输出Token
    - 指模型在一次回复中能生成的最大内容长度
    - 可以在API请求中手动设置max_output_token
    - 设置过低会导致回答不完整，内容被突然截断
    - 设置过高可能会增加导致API调用时间和费用

## 问题
1、为什么有些不行不支持长上下文
模型的上下文长度是指单词推理过程中能处理的最大输入和输出token序列的长度。这一参数在模型设计的时候会根据多个因素来确定：
- 首先，计算资源限制，因为Transformer架构的计算复杂度为O($n^{2}$)，其中n为序列长度。这意味着上下文长度越长，所需要的计算量和内存开销呈平方级增长，从而影响训练和推理效率。
- 其次，训练复杂度也制约了上下文长度，模型通常在训练阶段直接处理设定长度的序列，较长的序列需要在训练阶段梯度累计等技术来应对。
- 此外，位置编码和注意力机制的限制也影响了上下文长度的扩展性
2、模型推理中的Speculative Sampling
3、百炼的模型比自己本地部署快很多，差异在哪里？